{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourcing SCOTUS from Harvard's [Caselaw Access Project (CAP)](https://case.law/)\n",
    "\n",
    "Goal: retrieve all opinions written by the Supreme Court for a specified year range.\n",
    "\n",
    "SCOTUS denies thousands of cases every year, and each denial gets its own document, so we can't just grab all SCOTUS documents from CAP for a specified year. We need docket numbers for the cases that granted cert and argued before the court. Here, we source those docket numbers from the [Super-SCOTUS dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/POWQIT) [[paper](https://aclanthology.org/2023.nllp-1.20/)].\n",
    "\n",
    "1. Get docket numbers for the years 1986-2019 from superscotus.\n",
    "2. For each year, request a small sample (~15) cases from CAP. (waiting on unmetered API access before pulling full set)\n",
    "\n",
    "**Case issues**\n",
    "- [Board of Education v. Tom F.](https://cite.case.law/us/552/1/) Here, there was a recusal, and the court split 4-4, leading to a ~2-sentence per curiam opinion saying the lower court was affirmed by default. For some reason, you can't search for this case by docket number (via web or API)\n",
    "- [Altantic Sounding Co. v. Townsend](https://cite.case.law/us/557/404/): Classified as 11th circuit instead of SCOTUS, so 9009 court filter returns 0 results with this case's docket number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd -q ../..\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "from scotus_metalang import cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docket_nums_by_year = defaultdict(list)\n",
    "with jsonlines.open(\"data/super_scotus/1986_to_2019.jsonl\", \"r\") as f:\n",
    "    for case in f:\n",
    "        # Example case id: \"1986_84-2022\"\n",
    "        year = case[\"year\"]\n",
    "        docket_number = case[\"id\"][5:]\n",
    "        docket_nums_by_year[year].append(docket_number)\n",
    "fourteen_from_each = []\n",
    "for year in range(1986,2020):\n",
    "    fourteen_from_each += docket_nums_by_year[year][:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: turn into func and allow reprocess functionality to re-download data already logged.\n",
    "\n",
    "log_dir = Path(\"data/logs\")\n",
    "Path.mkdir(log_dir, exist_ok=True, parents=True)\n",
    "log_path = Path(log_dir, \"cap_scraping_log.tsv\")\n",
    "log_exists = os.path.exists(log_path)\n",
    "\n",
    "with open(log_path, \"r+\") as f:\n",
    "    header = [\"docket_number\", \"status\", \"cases_returned\", \"case_id_selected\", \"num_opinions\", \"authors\"]\n",
    "    reader = csv.DictReader(f, header, delimiter=\"\\t\")\n",
    "    # Add each TSV row to a dict indexed by docket number\n",
    "    log = {}\n",
    "    for line in reader:\n",
    "        docket_number = int(line.pop(\"docket_number\"))\n",
    "        log[docket_number] = line\n",
    "    writer = csv.DictWriter(f, header, delimiter=\"\\t\")\n",
    "    if not log_exists:\n",
    "        writer.writeheader()\n",
    "    for docket_number in fourteen_from_each[:5]:  # Sample here to limit API usage while tinkering\n",
    "        if docket_number in log:\n",
    "            continue  # Docket number already processed if in log\n",
    "        api_response = cap.cases_by_docket_number(docket_number)\n",
    "        count = api_response[\"count\"]\n",
    "        if count == 0:  # Case not found\n",
    "            writer.writerow({\"docket_number\": docket_number, \"status\": \"not_found\", \"cases_returned\": count,\n",
    "                             \"case_id_selected\": None, \"num_opinions\": None, \"authors\": None})\n",
    "            continue\n",
    "\n",
    "        case_id = cap.id_of_longest_casebody(api_response)\n",
    "        case_json = cap.case_json_by_id(case_id)\n",
    "        status = case[\"casebody\"][\"status\"]\n",
    "        if status == \"ok\":\n",
    "            try:\n",
    "                # Check expected keys exist\n",
    "                num_opinions = len(case_json[\"casebody\"][\"data\"][\"opinions\"])\n",
    "            except KeyError as err:\n",
    "                print(f\"{docket_number} num_opinions not accessible\")\n",
    "                continue\n",
    "\n",
    "            save_status = \"\"  # 'success' or will tell us which opinions are missing authors\n",
    "            for i, opinion in enumerate(case_json[\"casebody\"][\"data\"][\"opinions\"]):\n",
    "                authors = []\n",
    "                save_result = cap.save_opinion(case_id, docket_number,\n",
    "                                               api_response[\"decision_date\"], opinion, i)\n",
    "                if save_result[\"status\"] == \"success\":\n",
    "                    authors.append(save_result[\"author\"])\n",
    "                else:\n",
    "                    save_status += save_result[\"status\"]\n",
    "            if save_status == \"\":\n",
    "                save_status = \"success\"\n",
    "            else:\n",
    "                save_status = save_status[:-1]  # Remove trailing comma from last opinion with missing author\n",
    "            writer.writerow({\"docket_number\": docket_number, \"status\": save_status, \"cases_returned\": count,\n",
    "                             \"case_id_selected\": case_id, \"num_opinions\": num_opinions, \"authors\": \"|\".join(authors)})\n",
    "        elif status == \"error_limit_exceeded\":\n",
    "            writer.writerow({\"docket_number\": docket_number, \"status\": \"limit_exceeded\", \"cases_returned\": count,\n",
    "                             \"case_id_selected\": case_id, \"num_opinions\": None, \"authors\": None})\n",
    "\n",
    "        else:\n",
    "            writer.writerow({\"docket_number\": docket_number, \"status\": status, \"cases_returned\": count,\n",
    "                             \"case_id_selected\": case_id, \"num_opinions\": None, \"authors\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cases:\n",
    "    if list(x.keys()[0]) != \"id\":\n",
    "        for opinion in x[\"casebody\"][\"data\"][\"opinions\"]:\n",
    "            if opinion[\"author\"] is None:    \n",
    "                print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"data/harvard_cap/14_cases_from_1986_to_2019.jsonl\"\n",
    "with jsonlines.open(target, \"w\") as f:\n",
    "    f.write_all(cases)\n",
    "    print(f\"{len(cases)} written to {target}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scotus-metalang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
